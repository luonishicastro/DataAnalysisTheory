{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explain the basics on MapReduce.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>The idea of MapReduce is to break work into mappers that can run in parallel and reducers that take the output of mappers and process it. The first operation is called \"mapping\" because it takes each element of input data and maps a function onto int, leaving the output of the reducer to handle.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explain the basics about the Hadoop File System.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>A special filesystem is needed to provide to provide data efficiently to MapReduce, and the most popular one is the Hadoop File System (HDFS). It is massively parallel, highly available, self-healing filesystem. Like many other NoSQL databases, HDFS is a sophisticated kind of key/value store.\n",
    "<br>\n",
    "It makes multiple copies of each block (by default, three copies) and stores these copies on different nodes. This way, if one node dies, two other copies are still available and the block will be copied to a third node once the failure is detected without affecting availability. The multiple copies also facilitate load balancing, because we can choose to send the work to the least busy node that contains the data.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describe the shuffle step of MapReduce.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>In order to make sure that the same reducer gets all the blocks for a single file, we will leverage what's called the shuffle step of MapReduce, where the output of the mappers contains a key and a value and a shuffle function is applied to the key to send all work with the same key to the same reducer.\n",
    "<br>\n",
    "When there are multiple reducers, in order to create a single file we will need to channel all of the reducers to a single reducer that will assemble the final output into one file, adding complexity and processing time. Instead, to optimize for multiple reducers working in parallel, most MapReduce jobs generate multiple files, typically in the same directory.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describe the concept of sequence files in Hadoop.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Because so much work is done on the block level, Hadoop usually has a large block size, defaulting to 64 or 128 MB. Since only one file can be stored in a block, this makes Hadoop not very efficient for storing small files - a 1 KB file would still take a whole block. To optimize storage, sequence files were introduced. A sequence file is a collection of key/value pairs and is often used to store lots of small files in one large file by using smaller files names as the keys and the contents as the values.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explain the basics of Spark.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
